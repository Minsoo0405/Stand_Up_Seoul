{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "import joblib\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "def get_score(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test,y_pred)\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "    \n",
    "    return(mae, mse, rmse, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "#data = pd.read_csv('2008~2023_data.csv', engine='python')\n",
    "data = pd.read_parquet('2008~2023_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 사이즈가 너무 크기 때문에 샘플데이터를 통하여 모델 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['year'] >= 2018) & (data['year'] <= 2023)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파생변수 생성\n",
    "- 비가 온날과 안온날 차이가 있었기 때문에 rainy 파생변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rainy'] = data['Rainfall_amt']>0.0\n",
    "data['rainy'] = data['rainy'].astype(np.int32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 종속변수가 최대한 정규분포를 따라야 하고 이상치를 최소화 하기위해 로그 스케일링 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_get_all'] = np.log1p(data['get_all'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 푸리에특징을 통한 시간연속성 표현(HOUR)\n",
    "- 간단한 푸리에 변환을 활용하여, hour의 시간연속성을 데이터에 표현하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_encoding(x, max_val):\n",
    "    sin_val = np.sin(2 * np.pi * x / max_val)\n",
    "    cos_val = np.cos(2 * np.pi * x / max_val)\n",
    "    return sin_val, cos_val\n",
    "\n",
    "# hour 변수를 Cyclical Encoding으로 변환하여 대체하기\n",
    "max_hour = 24\n",
    "data['hour_sin'], data['hour_cos']= cyclical_encoding(data['hour'], max_hour)\n",
    "data.drop('hour', axis=1, inplace=True)\n",
    "\n",
    "# 결과 확인\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원핫인코딩 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "selected_columns = ['log_get_all', 'holiday', 'rainy', 'hour_sin', 'hour_cos']\n",
    "categorical_columns = ['Station_num', 'Line_num', 'weekday', 'month']\n",
    "\n",
    "# Apply one-hot encoding to categorical columns\n",
    "encoded_columns = []\n",
    "for column in categorical_columns:\n",
    "    encoded_df = pd.get_dummies(data[column], prefix=column, prefix_sep='_')\n",
    "    encoded_columns.append(encoded_df)\n",
    "\n",
    "# Encode the 'year' column separately\n",
    "year_data = pd.get_dummies(data['year'], prefix='year', prefix_sep='_')\n",
    "encoded_columns.append(year_data)\n",
    "\n",
    "# Concatenate the selected columns and encoded columns\n",
    "data_encoded = pd.concat([data[selected_columns]] + encoded_columns, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 데이터셋과 테스트 데이터셋 나누기\n",
    "- 2018년부터 2021년까지의 데이터를 훈련 데이터로 사용하고, 2022년부터 2023년까지의 데이터를 테스트 데이터로 사용함. 이를 통해 모델은 과거 데이터를 기반으로 학습하여 미래의 데이터에 대해 예측 수행 할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the start and end years for training and testing\n",
    "train_start_year = 2018\n",
    "train_end_year = 2021\n",
    "test_start_year = 2022\n",
    "test_end_year = 2023\n",
    "\n",
    "# Filter the data based on the years\n",
    "train_data = data_encoded[(data['year'] >= train_start_year) & (data['year'] <= train_end_year)]\n",
    "test_data = data_encoded[(data['year'] >= test_start_year) & (data['year'] <= test_end_year)]\n",
    "\n",
    "# Separate the features (X) and target variable (Y)\n",
    "X_train = train_data.drop(['log_get_all'], axis=1)\n",
    "Y_train = train_data['log_get_all']\n",
    "X_test = test_data.drop(['log_get_all'], axis=1)\n",
    "Y_test = test_data['log_get_all']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# catboost 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TimeSeriesSplit object for time series splitting\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Variable initialization\n",
    "log_mae_scores = []\n",
    "mae_scores = []\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "r2_scores = []\n",
    "train_r2_scores = []\n",
    "\n",
    "# Create a list of models\n",
    "models = [\n",
    "    CatBoostRegressor(random_state=42),\n",
    "]\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Outer loop: iterate over the time series segmentation\n",
    "for train_index, test_index in tscv.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
    "\n",
    "    # Inner loop: estimate the model's performance through cross-validation\n",
    "    for model in models:\n",
    "        model.fit(X_train_fold, y_train_fold)  # Train the model\n",
    "\n",
    "        scores = -cross_val_score(model, X_train_fold, y_train_fold, scoring='neg_mean_absolute_error', cv=5)\n",
    "        log_mean_score = scores.mean()\n",
    "        log_mae_scores.append(log_mean_score)\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        y_pred = np.expm1(model.predict(X_val_fold))\n",
    "        mae = mean_absolute_error(np.expm1(y_val_fold), y_pred)\n",
    "        mae_scores.append(mae)\n",
    "\n",
    "        # Evaluate the model on the training set\n",
    "        train_pred = np.expm1(model.predict(X_train_fold))\n",
    "        train_mae = mean_absolute_error(np.expm1(y_train_fold), train_pred)\n",
    "        train_scores.append(train_mae)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_pred = np.expm1(model.predict(X_test))\n",
    "        test_mae = mean_absolute_error(np.expm1(Y_test), test_pred)\n",
    "        test_scores.append(test_mae)\n",
    "\n",
    "        # Calculate R2 score on the training set\n",
    "        train_r2 = r2_score(np.expm1(y_train_fold), train_pred)\n",
    "        train_r2_scores.append(train_r2)\n",
    "\n",
    "        # Calculate R2 score on the test set\n",
    "        r2 = r2_score(np.expm1(Y_test), test_pred)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "        # Save the trained model using joblib\n",
    "        # joblib.dump(model, f'catboost_model_0623_ver2.pkl')\n",
    "\n",
    "        # Store the results in a dictionary\n",
    "        result = {\n",
    "            'Model': model.__class__.__name__,\n",
    "            'Log MAE (CV)': log_mean_score,\n",
    "            'MAE (CV)': mae,\n",
    "            'MAE (Train)': train_mae,\n",
    "            'MAE (Test)': test_mae,\n",
    "            'R2 Score (Train)': train_r2,\n",
    "            'R2 Score (Test)': r2\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f'{model.__class__.__name__}모델 학습 완료, TEST MAE : {test_mae}, TEST R2 : {r2}')\n",
    "        print('='*50)\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CatBoostClassifier model\n",
    "catboost_model = CatBoostRegressor(random_state=42)\n",
    "catboost_model.fit(X_train, Y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = catboost_model.get_feature_importance()\n",
    "\n",
    "# Get original feature names (without one-hot encoding)\n",
    "original_feature_names = [col.split('_')[0] for col in X_train.columns]\n",
    "\n",
    "# Create a dictionary to store the combined feature importances\n",
    "combined_feature_importances = {}\n",
    "\n",
    "# Iterate over the feature importances and sum them for the original feature names\n",
    "for feature_name, importance in zip(X_train.columns, feature_importances):\n",
    "    original_feature_name = feature_name.split('_')[0]\n",
    "    if original_feature_name in combined_feature_importances:\n",
    "        combined_feature_importances[original_feature_name] += importance\n",
    "    else:\n",
    "        combined_feature_importances[original_feature_name] = importance\n",
    "\n",
    "# Convert the combined feature importances dictionary to a DataFrame\n",
    "importance_df = pd.DataFrame.from_dict(combined_feature_importances, orient='index', columns=['Importance'])\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importance_df)), importance_df['Importance'], align='center')\n",
    "plt.yticks(range(len(importance_df)), importance_df.index)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Combined Feature Importances (One-Hot Encoded Variables)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
