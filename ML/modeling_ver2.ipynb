{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "def get_score(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test,y_pred)\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "    \n",
    "    return(mae, mse, rmse, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "#data = pd.read_csv('2008~2023_data.csv', engine='python')\n",
    "data = pd.read_parquet('2008~2023_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['year'] >= 2018) & (data['year'] <= 2023)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rainy'] = data['Rainfall_amt']>0.0\n",
    "data['rainy'] = data['rainy'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_get_all'] = np.log1p(data['get_all'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 푸리에특징을 통한 시간연속성 표현(HOUR)\n",
    "- 간단한 푸리에 변환을 활용하여, hour의 시간연속성을 데이터에 표현하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_encoding(x, max_val):\n",
    "    sin_val = np.sin(2 * np.pi * x / max_val)\n",
    "    cos_val = np.cos(2 * np.pi * x / max_val)\n",
    "    return sin_val, cos_val\n",
    "\n",
    "# hour 변수를 Cyclical Encoding으로 변환하여 대체하기\n",
    "max_hour = 24\n",
    "data['hour_sin'], data['hour_cos']= cyclical_encoding(data['hour'], max_hour)\n",
    "data.drop('hour', axis=1, inplace=True)\n",
    "\n",
    "# 결과 확인\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원핫인코딩 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역번호 원핫인코딩을 위한 데이터 프레임 만들기\n",
    "sub_num_data = pd.get_dummies(data['Station_num'], prefix='sub_num', prefix_sep='_')\n",
    "\n",
    "# 호선 원핫인코딩을 위한 데이터 프레임 만들기\n",
    "line_num_data = pd.get_dummies(data['Line_num'], prefix='line_num', prefix_sep='_')\n",
    "\n",
    "# 년도 원핫인코딩을 위한 데이터 프레임 만들기\n",
    "# year_data = pd.get_dummies(data['year'], prefix='year', prefix_sep='_')\n",
    "\n",
    "# 요일별 원핫인코딩을 위한 데이터 프레임 만들기\n",
    "weekday_data = pd.get_dummies(data['weekday'], prefix='weekday', prefix_sep='_')\n",
    "\n",
    "# 월별 원핫인코딩을 위한 데이터 프레임 만들기\n",
    "month_data = pd.get_dummies(data['month'], prefix='month', prefix_sep='_')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['log_get_all','holiday','rainy','hour_sin','hour_cos','year']\n",
    "data = pd.concat([data[selected_columns],  sub_num_data, line_num_data,\n",
    "                        weekday_data, month_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sub_num_data, line_num_data, weekday_data, month_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {}\n",
    "for i in data.columns:\n",
    "    dict_[i] = 0\n",
    "\n",
    "print(dict_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독립변수, 종속변수 설정 하기 \n",
    "X = data.drop(['log_get_all'],axis=1)\n",
    "Y = data['log_get_all']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# catboost 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Specify the start and end years for training and testing\n",
    "train_start_year = 2018\n",
    "train_end_year = 2021\n",
    "test_start_year = 2022\n",
    "test_end_year = 2023\n",
    "\n",
    "# Filter the data based on the years\n",
    "train_data = data[(data['year'] >= train_start_year) & (data['year'] <= train_end_year)]\n",
    "test_data = data[(data['year'] >= test_start_year) & (data['year'] <= test_end_year)]\n",
    "\n",
    "# Separate the features (X) and target variable (Y)\n",
    "X_train = train_data.drop(['log_get_all'], axis=1)\n",
    "Y_train = train_data['log_get_all']\n",
    "X_test = test_data.drop(['log_get_all'], axis=1)\n",
    "Y_test = test_data['log_get_all']\n",
    "\n",
    "# Create a TimeSeriesSplit object for time series splitting\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Variable initialization\n",
    "log_mae_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Create a list of models\n",
    "models = [\n",
    "    CatBoostRegressor(depth=12, iterations=1000, learning_rate=0.03, verbose=1, random_state=42),\n",
    "]\n",
    "\n",
    "# Outer loop: iterate over the time series segmentation\n",
    "for train_index, test_index in tscv.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
    "    \n",
    "    # Inner loop: estimate the model's performance through cross-validation\n",
    "    for model in models:\n",
    "        model.fit(X_train_fold, y_train_fold)  # Train the model\n",
    "        \n",
    "        scores = cross_val_score(model, X_train_fold, y_train_fold, scoring='neg_mean_absolute_error', cv=5)\n",
    "        log_mean_score = -scores.mean()  # Negate the scores since cross_val_score returns negative values\n",
    "        log_mae_scores.append(log_mean_score)\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        y_pred = np.expm1(model.predict(X_val_fold))\n",
    "        mae = mean_absolute_error(np.expm1(y_val_fold), y_pred)\n",
    "        mae_scores.append(mae)\n",
    "        \n",
    "        # Save the trained model using joblib\n",
    "        joblib.dump(model, f'catboost_model_0623_ver1.pkl')\n",
    "\n",
    "        print(f'{model.__class__.__name__} :')\n",
    "        print('Log MAE:', log_mean_score)\n",
    "        print('MAE:', mae)\n",
    "        print('=' * 50)\n",
    "\n",
    "# Visualize the results\n",
    "model_names = [model.__class__.__name__ for model in models]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, log_mae_scores, label='Log MAE')\n",
    "plt.bar(model_names, mae_scores, label='MAE')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.title('Comparison of Mean Absolute Error (MAE)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CatBoostRegressor model\n",
    "catboost_model = CatBoostRegressor(depth=12, iterations=1000, learning_rate=0.03, verbose=1, random_state=42)\n",
    "catboost_model.fit(X_train, Y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = catboost_model.get_feature_importance()\n",
    "\n",
    "# Get original feature names (without one-hot encoding)\n",
    "original_feature_names = [col.split('_')[0] for col in X_train.columns]\n",
    "\n",
    "# Create a dictionary to store the combined feature importances\n",
    "combined_feature_importances = {}\n",
    "\n",
    "# Iterate over the feature importances and sum them for the original feature names\n",
    "for feature_name, importance in zip(X_train.columns, feature_importances):\n",
    "    original_feature_name = feature_name.split('_')[0]\n",
    "    if original_feature_name in combined_feature_importances:\n",
    "        combined_feature_importances[original_feature_name] += importance\n",
    "    else:\n",
    "        combined_feature_importances[original_feature_name] = importance\n",
    "\n",
    "# Convert the combined feature importances dictionary to a DataFrame\n",
    "importance_df = pd.DataFrame.from_dict(combined_feature_importances, orient='index', columns=['Importance'])\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importance_df)), importance_df['Importance'], align='center')\n",
    "plt.yticks(range(len(importance_df)), importance_df.index)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Combined Feature Importances (One-Hot Encoded Variables)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
