{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import boto3\n",
    "from plugins import connector\n",
    "    \n",
    "def extract_data(cur, sql):\n",
    "    cur.execute(sql)\n",
    "    columns = [desc[0] for desc in cur.description]\n",
    "    results = cur.fetchall()\n",
    "    print(f'{len(results)}행 추출 완료')\n",
    "    df = pd.DataFrame(results)\n",
    "    df.columns = columns\n",
    "    print('데이터프레임 생성 완료')\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDSHIFT 데이터 추출\n",
    "데이터 웨어하우스에 적재된 population 데이터와 weather 데이터를 join하여 데이터를 추출하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, cur = connector.redshift_connector()\n",
    "\n",
    "# sql 수정 후에 사용해도 좋음\n",
    "# 내가 생각하기에 쓸모 없다고 생각하는 데이터는 뺐음\n",
    "sql = '''\n",
    "    SELECT\n",
    "        P.place_id,\n",
    "        P.area_congest_id,\n",
    "        P.area_ppltn_min,\n",
    "        P.area_ppltn_max,\n",
    "        W.temp,\n",
    "        W.sensible_temp,\n",
    "        W.humidity,\n",
    "        W.wind_dirct,\n",
    "        W.wind_spd,\n",
    "        W.precipitation,\n",
    "        W.uv_index_lvl,\n",
    "        W.pm25,\n",
    "        W.pm10,\n",
    "        W.air_idx_mvl,\n",
    "        W.created_date,\n",
    "        DATE_PART('year', W.created_date) AS year,\n",
    "        DATE_PART('month', W.created_date) AS month,\n",
    "        DATE_PART('day', W.created_date) AS day,\n",
    "        DATE_PART('hour', W.created_date) AS hour,\n",
    "        DATE_PART('minute', W.created_date) AS minute,\n",
    "        EXTRACT(DOW FROM W.created_date) AS dow\n",
    "    FROM\n",
    "        \"raw\".\"population\" AS P\n",
    "    JOIN\n",
    "        \"raw\".\"weather\" AS W ON P.place_id = W.place_id AND P.created_date = W.created_date;\n",
    "    '''\n",
    "    \n",
    "df = extract_data(cur, sql)\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측에 사용할 수 없는 변수 삭제 / area_congest_id - 1(xgboost 관련)\n",
    "- 데이터가 한정되어 있어, year, month는 일년동안 데이터를 쌓지 않는 이상 제대로 된 변수로 사용될 수 없을 것 같아 drop 하였습니다.\n",
    "- area_ppltn_min, area_ppltn_max, sensible_temp, uv_index_lvl, pm25, pm10, air_idx_mvl 컬럼은 예측 시점의 데이터를 가져오는 것이 불가능하기에 제외하였습니다.\n",
    "- wind_dirct의 경우 타겟변수와 유의미한 상관관계도 발견되지 않아, 제외하였습니다.\n",
    "- xgboost의 타겟변수 설정관련하여, area_congest_id를 0부터 시작하도록 조정하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cannotuse_columns = ['year', 'month', 'area_ppltn_min', 'area_ppltn_max', 'sensible_temp', 'wind_dirct', 'uv_index_lvl', 'pm25', 'pm10', 'air_idx_mvl']\n",
    "df = df.drop(columns = cannotuse_columns)\n",
    "\n",
    "# area_congest_id 값 1 빼기\n",
    "df['area_congest_id'] = df['area_congest_id'] - 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 푸리에특징을 통한 시간연속성 표현(HOUR)\n",
    "- 간단한 푸리에 변환을 활용하여, hour의 시간연속성을 데이터에 표현하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 관련 정보 추출\n",
    "hour = df['hour'].values\n",
    "\n",
    "# 시간 정보를 하나의 특징으로 합치기\n",
    "time_feature = hour / 24\n",
    "\n",
    "# 주기성을 나타내는 푸리에 특징 계산\n",
    "time_rad = 2 * np.pi * time_feature\n",
    "\n",
    "fourier_features = np.column_stack([\n",
    "    np.cos(time_rad), np.sin(time_rad)\n",
    "])\n",
    "\n",
    "# 생성된 특징을 데이터프레임에 추가\n",
    "df['fourier_cos_time'] = fourier_features[:, 0]\n",
    "df['fourier_sin_time'] = fourier_features[:, 1]\n",
    "\n",
    "# 기존 시간 관련 컬럼 제거\n",
    "df = df.drop(columns=['day','hour','minute'])\n",
    "df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 휴일 컬럼 추가\n",
    "공휴일인 경우 1의 값을 가집니다. 아닌 경우는 모두 0입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 휴일 관련 컬럼을 정의합니다.\n",
    "holiday_list = joblib.load('C:/last_project/ML/data/holiday.pkl')\n",
    "\n",
    "# created_date 컬럼을 datetime 형식으로 변환\n",
    "df['created_date'] = pd.to_datetime(df['created_date'])\n",
    "\n",
    "# holiday인 값을 디폴트 값 0으로 설정\n",
    "df['holiday'] = 0\n",
    "\n",
    "# created_date가 holiday_list에 포함된 날짜인 경우, holiday 컬럼을 1로 변경\n",
    "df.loc[df['created_date'].dt.strftime('%Y-%m-%d').isin(holiday_list), 'holiday'] = 1\n",
    "\n",
    "df = df.drop(columns = 'created_date')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원핫인코딩\n",
    "범주형 변수인 place_id와 dow(요일)을 원핫인코딩 처리하였습니다. 그 외에는 전부 연속형 변수입니다.(타겟변수인 area_congest_id는 제외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_features = ['place_id', 'dow']\n",
    "\n",
    "# one-hot encoding 수행\n",
    "df = pd.get_dummies(df, columns=onehot_features, dummy_na=False)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습 및 예측\n",
    "- nested_cross_validation을 활용하여, 시계열 데이터의 특성을 살려 검증하였습니다.\n",
    "- 상위 모델로는 CATBOOST, XGBOOST, LGBM, RANDOMFOREST를 선정하였으며 해당 결과는 튜닝 전 모델의 예측 성능에 근거하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성과 타겟 데이터 분할\n",
    "X = df.drop(columns = 'area_congest_id')\n",
    "y = df['area_congest_id']\n",
    "\n",
    "# 변수 초기화\n",
    "accuracy_scores = []\n",
    "\n",
    "# 모델 리스트 생성\n",
    "models = [\n",
    "    LogisticRegression(random_state=42),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    CatBoostClassifier(random_state=42),\n",
    "    XGBClassifier(random_state=42),\n",
    "    LGBMClassifier(random_state=42),\n",
    "    SVC(random_state=42),\n",
    "    KNeighborsClassifier()\n",
    "]\n",
    "\n",
    "# 교차 검증 수행 및 결과 저장\n",
    "for model in models:\n",
    "    scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross validation\n",
    "    mean_score = scores.mean()\n",
    "    accuracy_scores.append(mean_score)\n",
    "    print(f'{model.__class__.__name__} : {mean_score}점')\n",
    "    \n",
    "\n",
    "# 결과 시각화\n",
    "model_names = [model.__class__.__name__ for model in models]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, accuracy_scores)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.title('Comparison of Mean Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATBOOST, LGBM, XGBOOST의 특성중요도\n",
    "- 특성중요도 분석을 통해, 세 모델 전부 강수량이 중요하지 않은 결과를 보였습니다. 이 부분은 한달간 비오는 날이 적었던 이유를 들어, 해당 컬럼은 남겨두는 것으로 정하였습니다.\n",
    "- 이 중 CATBOOST는 변수의 영향력을 골고루 판단하고, 특히 fourier 관련 변수의 중요도를 높게 선정하여 시간과 관련된 장소 혼잡도 예측 결과를 유의미하게 다뤘다고 판단하였습니다.\n",
    "- LGBM은 우월한 학습 속도 및 변수의 영향력을 골고루 판단하였지만, 분석가의 판단 하에 place 관련 변수 만큼 fourier 관련 변수(hour)의 중요도를 높게 보고 있기에, 우선순위는 CATBOOST에 두었습니다.\n",
    "- XGBOOST는 place 관련 변수에 의존하는 경향이 있어, 이번 예측 모델로는 제외하였습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CatBoostClassifier model\n",
    "catboost_model = CatBoostClassifier(random_state=42)\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = catboost_model.get_feature_importance()\n",
    "\n",
    "# Get original feature names (without one-hot encoding)\n",
    "original_feature_names = [col.split('_')[0] for col in X_train.columns]\n",
    "\n",
    "# Create a dictionary to store the combined feature importances\n",
    "combined_feature_importances = {}\n",
    "\n",
    "# Iterate over the feature importances and sum them for the original feature names\n",
    "for feature_name, importance in zip(X_train.columns, feature_importances):\n",
    "    original_feature_name = feature_name.split('_')[0]\n",
    "    if original_feature_name in combined_feature_importances:\n",
    "        combined_feature_importances[original_feature_name] += importance\n",
    "    else:\n",
    "        combined_feature_importances[original_feature_name] = importance\n",
    "\n",
    "# Convert the combined feature importances dictionary to a DataFrame\n",
    "importance_df = pd.DataFrame.from_dict(combined_feature_importances, orient='index', columns=['Importance'])\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importance_df)), importance_df['Importance'], align='center')\n",
    "plt.yticks(range(len(importance_df)), importance_df.index)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Combined Feature Importances (One-Hot Encoded Variables)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LGBM model\n",
    "lgbm_model = lgb.LGBMClassifier(random_state=42)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = lgbm_model.feature_importances_\n",
    "\n",
    "# Get original feature names (without one-hot encoding)\n",
    "original_feature_names = [col.split('_')[0] for col in X_train.columns]\n",
    "\n",
    "# Create a dictionary to store the combined feature importances\n",
    "combined_feature_importances = {}\n",
    "\n",
    "# Iterate over the feature importances and sum them for the original feature names\n",
    "for feature_name, importance in zip(X_train.columns, feature_importances):\n",
    "    original_feature_name = feature_name.split('_')[0]\n",
    "    if original_feature_name in combined_feature_importances:\n",
    "        combined_feature_importances[original_feature_name] += importance\n",
    "    else:\n",
    "        combined_feature_importances[original_feature_name] = importance\n",
    "\n",
    "# Convert the combined feature importances dictionary to a DataFrame\n",
    "importance_df = pd.DataFrame.from_dict(combined_feature_importances, orient='index', columns=['Importance'])\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importance_df)), importance_df['Importance'], align='center')\n",
    "plt.yticks(range(len(importance_df)), importance_df.index)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Combined Feature Importances (lgbm_model)')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBClassifier model\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "\n",
    "# Get original feature names (without one-hot encoding)\n",
    "original_feature_names = [col.split('_')[0] for col in X_train.columns]\n",
    "\n",
    "# Create a dictionary to store the combined feature importances\n",
    "combined_feature_importances = {}\n",
    "\n",
    "# Iterate over the feature importances and sum them for the original feature names\n",
    "for feature_name, importance in zip(X_train.columns, feature_importances):\n",
    "    original_feature_name = feature_name.split('_')[0]\n",
    "    if original_feature_name in combined_feature_importances:\n",
    "        combined_feature_importances[original_feature_name] += importance\n",
    "    else:\n",
    "        combined_feature_importances[original_feature_name] = importance\n",
    "\n",
    "# Convert the combined feature importances dictionary to a DataFrame\n",
    "importance_df = pd.DataFrame.from_dict(combined_feature_importances, orient='index', columns=['Importance'])\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importance_df)), importance_df['Importance'], align='center')\n",
    "plt.yticks(range(len(importance_df)), importance_df.index)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Combined Feature Importances (xgb_model)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
