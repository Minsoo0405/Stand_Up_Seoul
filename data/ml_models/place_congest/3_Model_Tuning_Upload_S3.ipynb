{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import boto3\n",
    "from plugins import connector\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "    \n",
    "def extract_data(cur, sql):\n",
    "    cur.execute(sql)\n",
    "    columns = [desc[0] for desc in cur.description]\n",
    "    results = cur.fetchall()\n",
    "    print(f'{len(results)}행 추출 완료')\n",
    "    df = pd.DataFrame(results)\n",
    "    df.columns = columns\n",
    "    print('데이터프레임 생성 완료')\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDSHIFT 데이터 추출\n",
    "데이터 웨어하우스에 적재된 population 데이터와 weather 데이터를 join하여 데이터를 추출하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, cur = connector.redshift_connector()\n",
    "\n",
    "# sql 수정 후에 사용해도 좋음\n",
    "# 내가 생각하기에 쓸모 없다고 생각하는 데이터는 뺐음\n",
    "sql = '''\n",
    "    SELECT\n",
    "        P.place_id,\n",
    "        P.area_congest_id,\n",
    "        P.area_ppltn_min,\n",
    "        P.area_ppltn_max,\n",
    "        W.temp,\n",
    "        W.sensible_temp,\n",
    "        W.humidity,\n",
    "        W.wind_dirct,\n",
    "        W.wind_spd,\n",
    "        W.precipitation,\n",
    "        W.uv_index_lvl,\n",
    "        W.pm25,\n",
    "        W.pm10,\n",
    "        W.air_idx_mvl,\n",
    "        W.created_date,\n",
    "        DATE_PART('year', W.created_date) AS year,\n",
    "        DATE_PART('month', W.created_date) AS month,\n",
    "        DATE_PART('day', W.created_date) AS day,\n",
    "        DATE_PART('hour', W.created_date) AS hour,\n",
    "        DATE_PART('minute', W.created_date) AS minute,\n",
    "        EXTRACT(DOW FROM W.created_date) AS dow\n",
    "    FROM\n",
    "        \"raw\".\"population\" AS P\n",
    "    JOIN\n",
    "        \"raw\".\"weather\" AS W ON P.place_id = W.place_id AND P.created_date = W.created_date;\n",
    "    '''\n",
    "    \n",
    "df = extract_data(cur, sql)\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측에 사용할 수 없는 변수 삭제\n",
    "- 데이터가 한정되어 있어, year, month는 일년동안 데이터를 쌓지 않는 이상 제대로 된 변수로 사용될 수 없을 것 같아 drop 하였습니다.\n",
    "- area_ppltn_min, area_ppltn_max, sensible_temp, uv_index_lvl, pm25, pm10, air_idx_mvl 컬럼은 예측 시점의 데이터를 가져오는 것이 불가능하기에 제외하였습니다.\n",
    "- wind_dirct의 경우 타겟변수와 유의미한 상관관계도 발견되지 않아, 제외하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cannotuse_columns = ['year', 'month', 'area_ppltn_min', 'area_ppltn_max', 'sensible_temp', 'wind_dirct', 'uv_index_lvl', 'pm25', 'pm10', 'air_idx_mvl']\n",
    "df = df.drop(columns = cannotuse_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 푸리에특징을 통한 시간연속성 표현(HOUR)\n",
    "- 간단한 푸리에 변환을 활용하여, hour의 시간연속성을 데이터에 표현하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 관련 정보 추출\n",
    "hour = df['hour'].values\n",
    "\n",
    "# 시간 정보를 하나의 특징으로 합치기\n",
    "time_feature = hour / 24\n",
    "\n",
    "# 주기성을 나타내는 푸리에 특징 계산\n",
    "time_rad = 2 * np.pi * time_feature\n",
    "\n",
    "fourier_features = np.column_stack([\n",
    "    np.cos(time_rad), np.sin(time_rad)\n",
    "])\n",
    "\n",
    "# 생성된 특징을 데이터프레임에 추가\n",
    "df['fourier_cos_time'] = fourier_features[:, 0]\n",
    "df['fourier_sin_time'] = fourier_features[:, 1]\n",
    "\n",
    "# 기존 시간 관련 컬럼 제거\n",
    "df = df.drop(columns=['day','hour','minute'])\n",
    "df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 휴일컬럼 추가\n",
    "공휴일인 경우 1의 값을 가집니다. 아닌 경우는 모두 0입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 휴일 관련 컬럼을 정의합니다.\n",
    "holiday_list = joblib.load('C:/last_project/ML/data/holiday.pkl')\n",
    "\n",
    "# created_date 컬럼을 datetime 형식으로 변환\n",
    "df['created_date'] = pd.to_datetime(df['created_date'])\n",
    "\n",
    "# holiday인 값을 디폴트 값 0으로 설정\n",
    "df['holiday'] = 0\n",
    "\n",
    "# created_date가 holiday_list에 포함된 날짜인 경우, holiday 컬럼을 1로 변경\n",
    "df.loc[df['created_date'].dt.strftime('%Y-%m-%d').isin(holiday_list), 'holiday'] = 1\n",
    "\n",
    "df = df.drop(columns = 'created_date')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원핫인코딩\n",
    "범주형 변수인 place_id와 dow(요일)을 원핫인코딩 처리하였습니다. 그 외에는 전부 연속형 변수입니다.(타겟변수인 area_congest_id는 제외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_features = ['place_id', 'dow']\n",
    "\n",
    "# one-hot encoding 수행\n",
    "df = pd.get_dummies(df, columns=onehot_features, dummy_na=False)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터 튜닝\n",
    "- 타겟변수의 불균형 문제를 해결하기 위해 SMOTE 기법과, 클래스별 가중치를 비교해본 결과, SMOTE 기법은 과적합 문제로 인해서 새로운 데이터를 잘 예측하지 못하는 특성이 있었습니다.\n",
    "- 시계열 데이터의 특성보단, hour 피처만 푸리에 변환을 하였고, 나머지 시계열 데이터의 특성은 없앴으며, 클래스의 가중치를 주었습니다.\n",
    "- 그리드 서치를 활용하여 검증하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def compute_class_weights(y):\n",
    "    class_counts = y.value_counts()\n",
    "    total_samples = len(y)\n",
    "    class_weights = {}\n",
    "    for class_label, count in class_counts.items():\n",
    "        weight = total_samples / (len(class_counts) * count)\n",
    "        class_weights[class_label] = weight\n",
    "    return class_weights\n",
    "\n",
    "# 특성과 타겟 데이터 분할\n",
    "X = df.drop(columns='area_congest_id')\n",
    "y = df['area_congest_id']\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "class_weights = compute_class_weights(y)\n",
    "\n",
    "# 모델 초기화\n",
    "model = CatBoostClassifier(random_state=42, class_weights=class_weights)\n",
    "\n",
    "# 하이퍼파라미터 그리드 준비\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'depth': [3, 5, 7],\n",
    "    'iterations': [300, 500, 1000],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'bagging_temperature': [0.5, 1, 1.5],\n",
    "    'random_strength': [0.5, 1, 2]\n",
    "}\n",
    "\n",
    "# 교차 검증과 하이퍼파라미터 튜닝 수행\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=kf)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# 최적의 모델과 하이퍼파라미터 출력\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# 교차 검증 결과 출력\n",
    "accuracy_scores = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# 각 폴드의 정확도 점수 출력\n",
    "for fold, accuracy in enumerate(accuracy_scores):\n",
    "    print(f\"Fold {fold+1} Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# # 훈련 데이터에 대한 성능 평가\n",
    "# accuracy_train = accuracy_score(y_resampled, y_pred_train)\n",
    "# precision_train = precision_score(y_resampled, y_pred_train, average='weighted')  # 다중 클래스 평가를 위해 average='weighted' 사용\n",
    "# recall_train = recall_score(y_resampled, y_pred_train, average='weighted')  # 다중 클래스 평가를 위해 average='weighted' 사용\n",
    "# f1_train = f1_score(y_resampled, y_pred_train, average='weighted')  # 다중 클래스 평가를 위해 average='weighted' 사용\n",
    "\n",
    "# # 테스트 데이터에 대한 성능 평가\n",
    "# y_pred_test = best_model.predict(X_test)\n",
    "# accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "# precision_test = precision_score(y_test, y_pred_test, average='weighted')  # 다중 클래스 평가를 위해 average='weighted' 사용\n",
    "# recall_test = recall_score(y_test, y_pred_test, average='weighted')  # 다중 클래스 평가를 위해 average='weighted' 사용\n",
    "# f1_test = f1_score(y_test, y_pred_test, average='weighted')  # 다중 클래스 평가를 위해 average='weighted' 사용\n",
    "\n",
    "# # 출력\n",
    "# print(\"Best Model Evaluation:\")\n",
    "# print(f\"Train Accuracy: {accuracy_train}\")\n",
    "# print(f\"Train Precision: {precision_train}\")\n",
    "# print(f\"Train Recall: {recall_train}\")\n",
    "# print(f\"Train F1 Score: {f1_train}\")\n",
    "# print(f\"Test Accuracy: {accuracy_test}\")\n",
    "# print(f\"Test Precision: {precision_test}\")\n",
    "# print(f\"Test Recall: {recall_test}\")\n",
    "# print(f\"Test F1 Score: {f1_test}\")\n",
    "\n",
    "# # 학습 곡선 그리기\n",
    "# from sklearn.model_selection import learning_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# train_sizes, train_scores, test_scores = learning_curve(best_model, X_resampled, y_resampled, cv=kf, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "# train_mean = np.mean(train_scores, axis=1)\n",
    "# train_std = np.std(train_scores, axis=1)\n",
    "# test_mean = np.mean(test_scores, axis=1)\n",
    "# test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(train_sizes, train_mean, label='Train Accuracy')\n",
    "# plt.plot(train_sizes, test_mean, label='Validation Accuracy')\n",
    "# plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "# plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "# plt.xlabel('Training Set Size')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Learning Curve')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Best Model: <catboost.core.CatBoostClassifier object at 0x0000025F30E7C250>\n",
    "# Best Parameters: {'depth': 7, 'iterations': 1000, 'learning_rate': 0.5}\n",
    "\n",
    "now = datetime.now().strftime('%Y%m%d')\n",
    "# 최적의 모델 저장\n",
    "joblib.dump(best_model, f'best_model_{now}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
